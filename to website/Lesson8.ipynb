{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade diffusers transformers accelerate mediapy peft\n",
    "!apt -y install -qq aria2\n",
    "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\n",
    "!git clone --recurse-submodules -j8 https://github.com/camenduru/gpt4all\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/aryan1107/gpt4all-llora/resolve/main/gpt4all-lora-quantized.bin -d /content/gpt4all/chat -o gpt4all-lora-quantized.bin\n",
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapy as media\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from diffusers import DiffusionPipeline, TCDScheduler\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Choose among 1, 2, 4 and 8:\n",
    "num_inference_steps = 8\n",
    "\n",
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "repo_name = \"ByteDance/Hyper-SD\"\n",
    "plural = \"s\" if num_inference_steps > 1 else \"\"\n",
    "ckpt_name = f\"Hyper-SDXL-{num_inference_steps}step{plural}-lora.safetensors\"\n",
    "device = \"cuda\"\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\n",
    "pipe.fuse_lora()\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stable Diffusion\n",
    "prompt = \"a photo of Pikachu fine dining with a view to the Eiffel Tower\"\n",
    "seed = random.randint(0, sys.maxsize)\n",
    "\n",
    "# Decrease eta (min: 0, max: 1.0) to get more details with multi-step inference:\n",
    "eta = 0.5\n",
    "\n",
    "images = pipe(\n",
    "    prompt = prompt,\n",
    "    num_inference_steps = num_inference_steps,\n",
    "    guidance_scale = 0.0,\n",
    "    eta = eta,\n",
    "    generator = torch.Generator(device).manual_seed(seed),\n",
    "    ).images\n",
    "\n",
    "print(f\"Prompt:\\t{prompt}\\nSeed:\\t{seed}\")\n",
    "media.show_images(images)\n",
    "images[0].save(\"output.jpg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#GPT-4\n",
    "%xterm\n",
    "# Please copy and paste the code below in a terminal window. To paste, use Ctrl+Shift+V\n",
    "# cd /content/gpt4all/chat;./gpt4all-lora-quantized-linux-x86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code:\n",
    "https://github.com/woctezuma/stable-diffusion-colab?tab=readme-ov-file\n",
    "https://github.com/camenduru/gpt4all-colab?tab=readme-ov-file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
